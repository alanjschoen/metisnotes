{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Sets\n",
    "\n",
    "# Cleaning\n",
    "## drop nans\n",
    "## filling in values\n",
    "# Model prep\n",
    "This section will cover functions and methods in the [sklearn.preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) module and patsy.\n",
    "\n",
    "## Encoding class variables\n",
    "### Binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=1.1)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [onehot](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.transform(X)\n",
    "\n",
    "Note that patsy does one-hot encoding automatically.\n",
    "\n",
    "## Train test split\n",
    "\n",
    "## Scaling\n",
    "### Standard scaling\n",
    "#### Quick and dirty [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import scale\n",
    "X_scaled = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The right way [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "You can normalize data so that the columns have a unit norm of 1 using `l2` normalization. This is necessary for many applications of linear algebra.  [`sklearn.preprocessing.normalize`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) is similar to [`np.linalg.norm`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "from sklearn.preprocessing import normalize\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "# Object\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm='l2').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling sparse data\n",
    "Scaling sparse data is a little more tricky, because you want to do it witout allocating a ton of memory.  Centering the data just won't do, because that would require us to fill in all the missing values with the mean.\n",
    "\n",
    "Fortunately, sklearn has a few methods that can work with sparse data.\n",
    "\n",
    "#### MaxAbs Scaler\n",
    "The MaxAbs scaler scales the values by the max absolute value.  It does not touch the minimum value because, in many cases, sparse matrices hold counts and the missing values are all 0s.  Min-max scaling would turn all the 1s into 0s, and we don't want that.\n",
    "\n",
    "We can use this scaler as a [`MaxAbsScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) object, or using the quick-and-dirty function [`maxabs_scale`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.maxabs_scale.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scale and StandardScaler\n",
    "By default, `scale` and `StandardScaler` will throw an error if you use them with sparse matrices, but you can still use them if you set `with_mean=False`.  This will scale by standard deviation, but will not center the data. This is pretty much meaningless unless the data is centered, so I would only use this in a case where the data was already centered, i.e. we have positive and negative values in roughly equal proportion, and missing values represent 0.\n",
    "\n",
    "### Scaling with outliers\n",
    "If you have a lot of extreme outliers, the usual methods will not give you good results.  In this case, you can use a [`RobustScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html), which uses the median and interquartile range.  There is also a quick and dirty function [`robust_scale `](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html).\n",
    "\n",
    "## Odds and ends\n",
    "There are a few things in model prep that are interesting, but I didn't want to go into, so I'll just point to them here.\n",
    "### [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order features\n",
    "[`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) will compute higher order features for you.  It has a few useful flags.\n",
    "`degree` sets the highest order of the polynomials.  `PolynomialFeatures` will compute all of the degrees from 0 to `degree`.\n",
    "\n",
    "`interaction_only` eliminates terms with only one feature, i.e. [a^2, b^2].  It does not exclude 0th or 1st order terms.\n",
    "\n",
    "`include_bias`, if set to `True` includes a column of 1s.  Generally, this is a good idea, but it's worth paying attention to whether any of the other preprocessing steps will add another column of ones.  You should always add 1s after computing higher-order features, or else you will end up with a bunch of unnecessary cross-terms.\n",
    "\n",
    "After applying this transform, you will probably want to know what your cross terms are.  Fortunately, `PolynomialFeatures` has a member function `get_feature_names` which takes an array of names of the old features, and will tell you the names of your new featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly_names = poly.get_feature_names(input_features=None) # Uses [x0, x1 ..] by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced classes: undersampling/oversampling\n",
    "## Dimensionality reduction\n",
    "### PCA\n",
    "### SVD\n",
    "# Regression\n",
    "## Models\n",
    "## Accuracy\n",
    "### Scores\n",
    "# Classification\n",
    "## models\n",
    "## accuracy\n",
    "### ROC analysis\n",
    "### Confusion matrix\n",
    "### Scores\n",
    "# Tuning\n",
    "## Pipelines\n",
    "[Pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) offer a convenient way to store all of the stages of a modeling process.  This is especially useful for tuning. \n",
    "## Parameter optimization\n",
    "## Feature selection\n",
    "### Recursive feature elimination\n",
    "### nbest\n",
    "#### linear corr\n",
    "#### mutual information\n",
    "#### distance correlation\n",
    "# Visualization\n",
    "## Regression line\n",
    "## Space segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
